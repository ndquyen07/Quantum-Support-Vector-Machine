{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2500ed8d",
   "metadata": {},
   "source": [
    "# Optimizer Comparison Analysis\n",
    "\n",
    "This notebook analyzes and compares the performance of different optimizers for TQFM training on the Digits dataset.\n",
    "\n",
    "**Optimizers compared:**\n",
    "- COBYLA (Constrained Optimization BY Linear Approximation)\n",
    "- SPSA (Simultaneous Perturbation Stochastic Approximation)\n",
    "- ADAM (Adaptive Moment Estimation)\n",
    "- L_BFGS_B (Limited-memory BFGS with bounds)\n",
    "- SLSQP (Sequential Least SQuares Programming)\n",
    "\n",
    "**Analysis includes:**\n",
    "1. Loss convergence curves\n",
    "2. Final loss values comparison\n",
    "3. Training time comparison\n",
    "4. Statistical analysis across multiple runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec365ea7",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from src.utils import class_similarity, calculate_accuracy\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ae609",
   "metadata": {},
   "source": [
    "## 2. Load Results from HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d916d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "results_dir = Path(\"../results_optimizer\")\n",
    "depth = 1\n",
    "ansatz = \"TwoLocal\"\n",
    "maxiter = 100000\n",
    "num_runs = 10\n",
    "\n",
    "# Optimizers to compare (only 3 optimizers)\n",
    "optimizers = [\"COBYLA\", \"SPSA\", \"ADAM\"]\n",
    "\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"Configuration: depth={depth}, ansatz={ansatz}, maxiter={maxiter}\")\n",
    "print(f\"Number of runs per optimizer: {num_runs}\")\n",
    "print(f\"Optimizers: {optimizers}\")\n",
    "\n",
    "# Load data labels for accuracy calculation\n",
    "print(f\"\\nLoading dataset...\")\n",
    "loaded_data = np.load('../data/digits_8features_data.npz')\n",
    "X_train = loaded_data['X_train']\n",
    "y_train = loaded_data['y_train']\n",
    "X_val = loaded_data['X_val']\n",
    "y_val = loaded_data['y_val']\n",
    "X_test = loaded_data['X_test']\n",
    "y_test = loaded_data['y_test']\n",
    "\n",
    "print(f\"Training X shape: {X_train.shape}, y shape: {y_train.shape}\")\n",
    "print(f\"Validation X shape: {X_val.shape}, y shape: {y_val.shape}\")\n",
    "print(f\"Testing X shape: {X_test.shape}, y shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b382097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results (models and kernels)\n",
    "optimizer_results = {}\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    optimizer_results[optimizer] = {\n",
    "        'loss_histories': [],\n",
    "        'final_losses': [],\n",
    "        'optimal_values': [],\n",
    "        'num_iterations': [],\n",
    "        'run_ids': [],\n",
    "        'kernels_train': [],\n",
    "        'kernels_val': [],\n",
    "        'kernels_test': [],\n",
    "        'separability_ratios': [],\n",
    "        'val_accuracies': [],\n",
    "        'test_accuracies': [],\n",
    "        'best_C_values': []\n",
    "    }\n",
    "    \n",
    "    for run_id in range(1, num_runs + 1):\n",
    "        # Load model\n",
    "        model_filename = f\"tqfm_digits_depth{depth}_ansatz{ansatz}_optimizer{optimizer}_iter{maxiter}_run{run_id}.pkl\"\n",
    "        model_filepath = results_dir / model_filename\n",
    "        \n",
    "        # Load kernels\n",
    "        kernel_filename = f\"kernels_digits_depth{depth}_ansatz{ansatz}_optimizer{optimizer}_iter{maxiter}_run{run_id}.npz\"\n",
    "        kernel_filepath = results_dir / kernel_filename\n",
    "        \n",
    "        if model_filepath.exists() and kernel_filepath.exists():\n",
    "            # Load model\n",
    "            with open(model_filepath, 'rb') as f:\n",
    "                tqfm = pickle.load(f)\n",
    "            \n",
    "            # Load kernels\n",
    "            kernels = np.load(kernel_filepath)\n",
    "            K_train = kernels['kernel_after']\n",
    "            K_val = kernels['kernel_val']\n",
    "            K_test = kernels['kernel_test']\n",
    "            \n",
    "            # Calculate separability ratio\n",
    "            _, _, _, sep_ratio = class_similarity(K_train, y_train)\n",
    "            \n",
    "            # Calculate accuracies with optimal C selection\n",
    "            val_acc, test_acc, best_C = calculate_accuracy(\n",
    "                K_train, K_val, K_test, y_train, y_val, y_test\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            optimizer_results[optimizer]['loss_histories'].append(tqfm.loss_history)\n",
    "            optimizer_results[optimizer]['final_losses'].append(tqfm.loss_history[-1])\n",
    "            optimizer_results[optimizer]['optimal_values'].append(tqfm.optimal_value)\n",
    "            optimizer_results[optimizer]['num_iterations'].append(len(tqfm.loss_history))\n",
    "            optimizer_results[optimizer]['run_ids'].append(run_id)\n",
    "            optimizer_results[optimizer]['kernels_train'].append(K_train)\n",
    "            optimizer_results[optimizer]['kernels_val'].append(K_val)\n",
    "            optimizer_results[optimizer]['kernels_test'].append(K_test)\n",
    "            optimizer_results[optimizer]['separability_ratios'].append(sep_ratio)\n",
    "            optimizer_results[optimizer]['val_accuracies'].append(val_acc)\n",
    "            optimizer_results[optimizer]['test_accuracies'].append(test_acc)\n",
    "            optimizer_results[optimizer]['best_C_values'].append(best_C)\n",
    "        else:\n",
    "            if not model_filepath.exists():\n",
    "                print(f\"Warning: Model file not found - {model_filename}\")\n",
    "            if not kernel_filepath.exists():\n",
    "                print(f\"Warning: Kernel file not found - {kernel_filename}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Loaded Results Summary:\")\n",
    "print(\"=\"*70)\n",
    "for optimizer in optimizers:\n",
    "    num_loaded = len(optimizer_results[optimizer]['loss_histories'])\n",
    "    print(f\"{optimizer:12s}: {num_loaded}/{num_runs} runs loaded\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8d101",
   "metadata": {},
   "source": [
    "## 3. Select Best Run for Each Optimizer\n",
    "\n",
    "For each optimizer, select the run with the lowest final loss (best performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9065d8",
   "metadata": {},
   "source": [
    "## 4. Accuracy Analysis - All Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b399a5",
   "metadata": {},
   "source": [
    "## 6. Compare Best Runs - Loss Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d75399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss convergence curves for best runs only\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "optimizer_colors = {\n",
    "    'COBYLA': 'blue',\n",
    "    'SPSA': 'green',\n",
    "    'ADAM': 'red'\n",
    "}\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    if optimizer in best_runs:\n",
    "        loss_hist = best_runs[optimizer]['loss_history']\n",
    "        run_id = best_runs[optimizer]['run_id']\n",
    "        \n",
    "        ax.plot(loss_hist, linewidth=2.5, \n",
    "               label=f'{optimizer} (Run {run_id})',\n",
    "               color=optimizer_colors[optimizer])\n",
    "\n",
    "ax.set_xlabel('Iteration', fontsize=14)\n",
    "ax.set_ylabel('Loss E(Î¸)', fontsize=14)\n",
    "ax.set_title('Optimizer Comparison: Best Run Loss Convergence', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=12, loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(bottom=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results_img/optimizer_best_runs_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved: results_img/optimizer_best_runs_convergence.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6893a",
   "metadata": {},
   "source": [
    "## 7. All Runs Visualization (with Best Highlighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcd330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all runs with best run highlighted for each optimizer\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, optimizer in enumerate(optimizers):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot all runs in light color\n",
    "    for run_idx, loss_hist in enumerate(optimizer_results[optimizer]['loss_histories']):\n",
    "        run_id = optimizer_results[optimizer]['run_ids'][run_idx]\n",
    "        is_best = (run_id == best_runs[optimizer]['run_id'])\n",
    "        \n",
    "        if is_best:\n",
    "            # Plot best run with thick line\n",
    "            ax.plot(loss_hist, linewidth=3, \n",
    "                   color=optimizer_colors[optimizer], \n",
    "                   label=f'Best (Run {run_id})', zorder=10)\n",
    "        else:\n",
    "            # Plot other runs with thin, transparent lines\n",
    "            ax.plot(loss_hist, linewidth=1, alpha=0.3, \n",
    "                   color=optimizer_colors[optimizer])\n",
    "    \n",
    "    ax.set_xlabel('Iteration', fontsize=12)\n",
    "    ax.set_ylabel('Loss E(Î¸)', fontsize=12)\n",
    "    ax.set_title(f'{optimizer}', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results_img/optimizer_all_runs_with_best.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved: results_img/optimizer_all_runs_with_best.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35b184",
   "metadata": {},
   "source": [
    "## 8. Final Loss Comparison - All Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abc3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot and bar plot showing all runs with best highlighted\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Box plot - All runs\n",
    "data_for_boxplot = []\n",
    "labels_for_boxplot = []\n",
    "for optimizer in optimizers:\n",
    "    if len(optimizer_results[optimizer]['final_losses']) > 0:\n",
    "        data_for_boxplot.append(optimizer_results[optimizer]['final_losses'])\n",
    "        labels_for_boxplot.append(optimizer)\n",
    "\n",
    "bp = ax1.boxplot(data_for_boxplot, labels=labels_for_boxplot, patch_artist=True)\n",
    "for patch, optimizer in zip(bp['boxes'], labels_for_boxplot):\n",
    "    patch.set_facecolor(optimizer_colors[optimizer])\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "# Add best run markers\n",
    "for idx, optimizer in enumerate(labels_for_boxplot):\n",
    "    best_loss = best_runs[optimizer]['final_loss']\n",
    "    ax1.scatter(idx + 1, best_loss, color='red', s=200, marker='*', \n",
    "               zorder=10, edgecolors='black', linewidths=1.5)\n",
    "\n",
    "ax1.set_ylabel('Final Loss E(Î¸)', fontsize=12)\n",
    "ax1.set_xlabel('Optimizer', fontsize=12)\n",
    "ax1.set_title('Final Loss Distribution (â˜… = Best Run)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bar plot - Compare best runs only\n",
    "best_losses = [best_runs[opt]['final_loss'] for opt in optimizers if opt in best_runs]\n",
    "best_labels = [opt for opt in optimizers if opt in best_runs]\n",
    "x_pos = np.arange(len(best_labels))\n",
    "\n",
    "bars = ax2.bar(x_pos, best_losses, alpha=0.8,\n",
    "               color=[optimizer_colors[opt] for opt in best_labels])\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, loss) in enumerate(zip(bars, best_losses)):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "            f'{loss:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax2.set_ylabel('Final Loss E(Î¸)', fontsize=12)\n",
    "ax2.set_xlabel('Optimizer', fontsize=12)\n",
    "ax2.set_title('Best Run Final Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(best_labels)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results_img/optimizer_final_loss_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved: results_img/optimizer_final_loss_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a5d0b8",
   "metadata": {},
   "source": [
    "## 9. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary with all metrics\n",
    "print(\"\\n\" + \"=\"*130)\n",
    "print(\"STATISTICAL SUMMARY - ALL RUNS\")\n",
    "print(\"=\"*130)\n",
    "\n",
    "stats_all_runs = []\n",
    "for optimizer in optimizers:\n",
    "    if len(optimizer_results[optimizer]['final_losses']) > 0:\n",
    "        stats_all_runs.append({\n",
    "            'Optimizer': optimizer,\n",
    "            'N': len(optimizer_results[optimizer]['final_losses']),\n",
    "            'Loss': f\"{np.mean(optimizer_results[optimizer]['final_losses']):.4f}Â±{np.std(optimizer_results[optimizer]['final_losses']):.4f}\",\n",
    "            'Sep Ratio': f\"{np.mean(optimizer_results[optimizer]['separability_ratios']):.4f}Â±{np.std(optimizer_results[optimizer]['separability_ratios']):.4f}\",\n",
    "            'Val Acc': f\"{np.mean(optimizer_results[optimizer]['val_accuracies']):.4f}Â±{np.std(optimizer_results[optimizer]['val_accuracies']):.4f}\",\n",
    "            'Test Acc': f\"{np.mean(optimizer_results[optimizer]['test_accuracies']):.4f}Â±{np.std(optimizer_results[optimizer]['test_accuracies']):.4f}\",\n",
    "            'Best C': f\"{np.mean(optimizer_results[optimizer]['best_C_values']):.2f}Â±{np.std(optimizer_results[optimizer]['best_C_values']):.2f}\"\n",
    "        })\n",
    "\n",
    "df_all_runs = pd.DataFrame(stats_all_runs)\n",
    "print(df_all_runs.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*130)\n",
    "print(\"BEST RUN COMPARISON\")\n",
    "print(\"=\"*130)\n",
    "\n",
    "stats_best_runs = []\n",
    "for optimizer in optimizers:\n",
    "    if optimizer in best_runs:\n",
    "        stats_best_runs.append({\n",
    "            'Optimizer': optimizer,\n",
    "            'Run ID': best_runs[optimizer]['run_id'],\n",
    "            'Loss': best_runs[optimizer]['final_loss'],\n",
    "            'Sep Ratio': best_runs[optimizer]['separability_ratio'],\n",
    "            'Val Acc': best_runs[optimizer]['val_accuracy'],\n",
    "            'Test Acc': best_runs[optimizer]['test_accuracy'],\n",
    "            'Best C': best_runs[optimizer]['best_C'],\n",
    "            'Iterations': best_runs[optimizer]['num_iterations']\n",
    "        })\n",
    "\n",
    "df_best_runs = pd.DataFrame(stats_best_runs)\n",
    "df_best_runs = df_best_runs.sort_values('Loss')\n",
    "print(df_best_runs.to_string(index=False))\n",
    "print(\"=\"*130)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b45033",
   "metadata": {},
   "source": [
    "## 10. Convergence Speed Analysis - Best Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze convergence speed for best runs\n",
    "thresholds = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n",
    "\n",
    "convergence_table = []\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONVERGENCE SPEED - Iterations to reach loss threshold\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    if optimizer in best_runs:\n",
    "        loss_hist = np.array(best_runs[optimizer]['loss_history'])\n",
    "        row = {'Optimizer': optimizer}\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            below_threshold = np.where(loss_hist < threshold)[0]\n",
    "            if len(below_threshold) > 0:\n",
    "                iterations = below_threshold[0]\n",
    "                row[f'{threshold}'] = iterations\n",
    "            else:\n",
    "                row[f'{threshold}'] = np.nan\n",
    "        \n",
    "        convergence_table.append(row)\n",
    "\n",
    "df_convergence = pd.DataFrame(convergence_table)\n",
    "print(df_convergence.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Plot convergence speed\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    if optimizer in best_runs:\n",
    "        iterations_list = []\n",
    "        for threshold in thresholds:\n",
    "            loss_hist = np.array(best_runs[optimizer]['loss_history'])\n",
    "            below_threshold = np.where(loss_hist < threshold)[0]\n",
    "            if len(below_threshold) > 0:\n",
    "                iterations_list.append(below_threshold[0])\n",
    "            else:\n",
    "                iterations_list.append(np.nan)\n",
    "        \n",
    "        ax.plot(thresholds, iterations_list, marker='o', linewidth=2.5, markersize=8,\n",
    "               label=f\"{optimizer} (Run {best_runs[optimizer]['run_id']})\",\n",
    "               color=optimizer_colors[optimizer])\n",
    "\n",
    "ax.set_xlabel('Loss Threshold', fontsize=14)\n",
    "ax.set_ylabel('Iterations to Reach Threshold', fontsize=14)\n",
    "ax.set_title('Convergence Speed Comparison - Best Runs', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.invert_xaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results_img/optimizer_convergence_speed.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFigure saved: results_img/optimizer_convergence_speed.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44dd04",
   "metadata": {},
   "source": [
    "## 11. Final Summary and Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b546615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL SUMMARY - OPTIMIZER COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "if len(df_best_runs) > 0:\n",
    "    best_optimizer = df_best_runs.iloc[0]['Optimizer']\n",
    "    best_run_id = df_best_runs.iloc[0]['Run ID']\n",
    "    best_loss = df_best_runs.iloc[0]['Loss']\n",
    "    best_test_acc = df_best_runs.iloc[0]['Test Acc']\n",
    "    best_sep_ratio = df_best_runs.iloc[0]['Sep Ratio']\n",
    "    \n",
    "    print(f\"\\nðŸ† BEST OPTIMIZER (by lowest loss): {best_optimizer}\")\n",
    "    print(f\"   Best Run ID: {best_run_id}\")\n",
    "    print(f\"   Final Loss: {best_loss:.6f}\")\n",
    "    print(f\"   Separability Ratio: {best_sep_ratio:.6f}\")\n",
    "    print(f\"   Test Accuracy: {best_test_acc:.4f} ({best_test_acc*100:.2f}%)\")\n",
    "    print(f\"   Best C parameter: {df_best_runs.iloc[0]['Best C']:.3f}\")\n",
    "    \n",
    "    # Find best by test accuracy\n",
    "    best_acc_idx = df_best_runs['Test Acc'].idxmax()\n",
    "    best_acc_optimizer = df_best_runs.loc[best_acc_idx, 'Optimizer']\n",
    "    best_acc_value = df_best_runs.loc[best_acc_idx, 'Test Acc']\n",
    "    best_acc_run = df_best_runs.loc[best_acc_idx, 'Run ID']\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ BEST OPTIMIZER (by test accuracy): {best_acc_optimizer}\")\n",
    "    print(f\"   Run ID: {best_acc_run}\")\n",
    "    print(f\"   Test Accuracy: {best_acc_value:.4f} ({best_acc_value*100:.2f}%)\")\n",
    "    print(f\"   Loss: {df_best_runs.loc[best_acc_idx, 'Loss']:.6f}\")\n",
    "    \n",
    "    # Find best by separability ratio\n",
    "    best_sep_idx = df_best_runs['Sep Ratio'].idxmax()\n",
    "    best_sep_optimizer = df_best_runs.loc[best_sep_idx, 'Optimizer']\n",
    "    best_sep_value = df_best_runs.loc[best_sep_idx, 'Sep Ratio']\n",
    "    \n",
    "    print(f\"\\nðŸ“Š BEST OPTIMIZER (by separability ratio): {best_sep_optimizer}\")\n",
    "    print(f\"   Separability Ratio: {best_sep_value:.6f}\")\n",
    "    print(f\"   Test Accuracy: {df_best_runs.loc[best_sep_idx, 'Test Acc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ RANKING BY METRICS:\")\n",
    "    print(f\"\\n   By Lowest Loss:\")\n",
    "    for idx, row in df_best_runs.iterrows():\n",
    "        print(f\"      {idx+1}. {row['Optimizer']:10s} - Loss: {row['Loss']:.6f}, Test Acc: {row['Test Acc']:.4f}\")\n",
    "    \n",
    "    df_by_acc = df_best_runs.sort_values('Test Acc', ascending=False)\n",
    "    print(f\"\\n   By Highest Test Accuracy:\")\n",
    "    for idx, row in df_by_acc.iterrows():\n",
    "        print(f\"      {idx+1}. {row['Optimizer']:10s} - Test Acc: {row['Test Acc']:.4f}, Loss: {row['Loss']:.6f}\")\n",
    "    \n",
    "    df_by_sep = df_best_runs.sort_values('Sep Ratio', ascending=False)\n",
    "    print(f\"\\n   By Highest Separability Ratio:\")\n",
    "    for idx, row in df_by_sep.iterrows():\n",
    "        print(f\"      {idx+1}. {row['Optimizer']:10s} - Sep: {row['Sep Ratio']:.6f}, Test Acc: {row['Test Acc']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n\udcc9 VARIABILITY ANALYSIS (across 10 runs):\")\n",
    "    for optimizer in optimizers:\n",
    "        if len(optimizer_results[optimizer]['test_accuracies']) > 0:\n",
    "            print(f\"   {optimizer:10s}:\")\n",
    "            \n",
    "            mean_loss = np.mean(optimizer_results[optimizer]['final_losses'])\n",
    "            std_loss = np.std(optimizer_results[optimizer]['final_losses'])\n",
    "            cv_loss = (std_loss/mean_loss*100) if mean_loss != 0 else 0\n",
    "            print(f\"      Loss:      {mean_loss:.6f} Â± {std_loss:.6f} (CV={cv_loss:.2f}%)\")\n",
    "            \n",
    "            mean_sep = np.mean(optimizer_results[optimizer]['separability_ratios'])\n",
    "            std_sep = np.std(optimizer_results[optimizer]['separability_ratios'])\n",
    "            cv_sep = (std_sep/mean_sep*100) if mean_sep != 0 else 0\n",
    "            print(f\"      Sep Ratio: {mean_sep:.6f} Â± {std_sep:.6f} (CV={cv_sep:.2f}%)\")\n",
    "            \n",
    "            mean_acc = np.mean(optimizer_results[optimizer]['test_accuracies'])\n",
    "            std_acc = np.std(optimizer_results[optimizer]['test_accuracies'])\n",
    "            cv_acc = (std_acc/mean_acc*100) if mean_acc != 0 else 0\n",
    "            print(f\"      Test Acc:  {mean_acc:.4f} Â± {std_acc:.4f} (CV={cv_acc:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ KEY FINDINGS:\")\n",
    "    print(f\"   1. Best overall performance: {best_optimizer} (Run {best_run_id})\")\n",
    "    print(f\"      - Lowest loss: {best_loss:.6f}\")\n",
    "    print(f\"      - Test accuracy: {best_test_acc:.4f}\")\n",
    "    print(f\"      - Separability ratio: {best_sep_ratio:.6f}\")\n",
    "    print(f\"   2. Higher separability ratio generally correlates with better accuracy\")\n",
    "    print(f\"   3. Lower loss generally leads to better separability and accuracy\")\n",
    "    print(f\"   4. Initial parameters significantly affect results (CV shows variability)\")\n",
    "    print(f\"   5. Optimal C parameter varies: {df_best_runs['Best C'].min():.3f} to {df_best_runs['Best C'].max():.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd12d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best run (lowest final loss) for each optimizer\n",
    "best_runs = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BEST RUN SELECTION (Lowest Final Loss)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    if len(optimizer_results[optimizer]['final_losses']) > 0:\n",
    "        # Find index of run with minimum final loss\n",
    "        best_idx = np.argmin(optimizer_results[optimizer]['final_losses'])\n",
    "        \n",
    "        best_runs[optimizer] = {\n",
    "            'run_id': optimizer_results[optimizer]['run_ids'][best_idx],\n",
    "            'loss_history': optimizer_results[optimizer]['loss_histories'][best_idx],\n",
    "            'final_loss': optimizer_results[optimizer]['final_losses'][best_idx],\n",
    "            'optimal_value': optimizer_results[optimizer]['optimal_values'][best_idx],\n",
    "            'num_iterations': optimizer_results[optimizer]['num_iterations'][best_idx],\n",
    "            'kernel_train': optimizer_results[optimizer]['kernels_train'][best_idx],\n",
    "            'kernel_val': optimizer_results[optimizer]['kernels_val'][best_idx],\n",
    "            'kernel_test': optimizer_results[optimizer]['kernels_test'][best_idx],\n",
    "            'separability_ratio': optimizer_results[optimizer]['separability_ratios'][best_idx],\n",
    "            'val_accuracy': optimizer_results[optimizer]['val_accuracies'][best_idx],\n",
    "            'test_accuracy': optimizer_results[optimizer]['test_accuracies'][best_idx],\n",
    "            'best_C': optimizer_results[optimizer]['best_C_values'][best_idx]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{optimizer}:\")\n",
    "        print(f\"  Best Run ID: {best_runs[optimizer]['run_id']}\")\n",
    "        print(f\"  Final Loss: {best_runs[optimizer]['final_loss']:.6f}\")\n",
    "        print(f\"  Separability Ratio: {best_runs[optimizer]['separability_ratio']:.6f}\")\n",
    "        print(f\"  Iterations: {best_runs[optimizer]['num_iterations']}\")\n",
    "        print(f\"  Best C: {best_runs[optimizer]['best_C']:.3f}\")\n",
    "        print(f\"  Val Accuracy: {best_runs[optimizer]['val_accuracy']:.4f}\")\n",
    "        print(f\"  Test Accuracy: {best_runs[optimizer]['test_accuracy']:.4f}\")\n",
    "        \n",
    "        # Show all runs for comparison\n",
    "        print(f\"  All runs (Loss | Sep Ratio | Test Acc):\")\n",
    "        for i, (run_id, loss, sep, test_acc) in enumerate(zip(\n",
    "                optimizer_results[optimizer]['run_ids'], \n",
    "                optimizer_results[optimizer]['final_losses'],\n",
    "                optimizer_results[optimizer]['separability_ratios'],\n",
    "                optimizer_results[optimizer]['test_accuracies'])):\n",
    "            marker = \" â† BEST\" if i == best_idx else \"\"\n",
    "            print(f\"    Run {run_id:2d}: {loss:.6f} | {sep:.6f} | {test_acc:.4f}{marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy and Separability comparison across all runs\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "optimizer_colors = {\n",
    "    'COBYLA': 'blue',\n",
    "    'SPSA': 'green',\n",
    "    'ADAM': 'red'\n",
    "}\n",
    "\n",
    "# 1. Validation Accuracy\n",
    "ax = axes[0, 0]\n",
    "data_box = [optimizer_results[opt]['val_accuracies'] for opt in optimizers if len(optimizer_results[opt]['val_accuracies']) > 0]\n",
    "labels_box = [opt for opt in optimizers if len(optimizer_results[opt]['val_accuracies']) > 0]\n",
    "bp = ax.boxplot(data_box, labels=labels_box, patch_artist=True)\n",
    "for patch, optimizer in zip(bp['boxes'], labels_box):\n",
    "    patch.set_facecolor(optimizer_colors[optimizer])\n",
    "    patch.set_alpha(0.6)\n",
    "for i, optimizer in enumerate(labels_box):\n",
    "    best_acc = best_runs[optimizer]['val_accuracy']\n",
    "    ax.scatter(i + 1, best_acc, color='red', s=200, marker='*', zorder=10, edgecolors='black', linewidths=1.5)\n",
    "ax.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax.set_title('Validation Accuracy (â˜… = Best Run)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Test Accuracy\n",
    "ax = axes[0, 1]\n",
    "data_box = [optimizer_results[opt]['test_accuracies'] for opt in optimizers if len(optimizer_results[opt]['test_accuracies']) > 0]\n",
    "labels_box = [opt for opt in optimizers if len(optimizer_results[opt]['test_accuracies']) > 0]\n",
    "bp = ax.boxplot(data_box, labels=labels_box, patch_artist=True)\n",
    "for patch, optimizer in zip(bp['boxes'], labels_box):\n",
    "    patch.set_facecolor(optimizer_colors[optimizer])\n",
    "    patch.set_alpha(0.6)\n",
    "for i, optimizer in enumerate(labels_box):\n",
    "    best_acc = best_runs[optimizer]['test_accuracy']\n",
    "    ax.scatter(i + 1, best_acc, color='red', s=200, marker='*', zorder=10, edgecolors='black', linewidths=1.5)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title('Test Accuracy (â˜… = Best Run)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Separability Ratio\n",
    "ax = axes[1, 0]\n",
    "data_box = [optimizer_results[opt]['separability_ratios'] for opt in optimizers if len(optimizer_results[opt]['separability_ratios']) > 0]\n",
    "labels_box = [opt for opt in optimizers if len(optimizer_results[opt]['separability_ratios']) > 0]\n",
    "bp = ax.boxplot(data_box, labels=labels_box, patch_artist=True)\n",
    "for patch, optimizer in zip(bp['boxes'], labels_box):\n",
    "    patch.set_facecolor(optimizer_colors[optimizer])\n",
    "    patch.set_alpha(0.6)\n",
    "for i, optimizer in enumerate(labels_box):\n",
    "    best_sep = best_runs[optimizer]['separability_ratio']\n",
    "    ax.scatter(i + 1, best_sep, color='red', s=200, marker='*', zorder=10, edgecolors='black', linewidths=1.5)\n",
    "ax.set_ylabel('Separability Ratio', fontsize=12)\n",
    "ax.set_title('Separability Ratio (â˜… = Best Run)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Best C Parameter\n",
    "ax = axes[1, 1]\n",
    "data_box = [optimizer_results[opt]['best_C_values'] for opt in optimizers if len(optimizer_results[opt]['best_C_values']) > 0]\n",
    "labels_box = [opt for opt in optimizers if len(optimizer_results[opt]['best_C_values']) > 0]\n",
    "bp = ax.boxplot(data_box, labels=labels_box, patch_artist=True)\n",
    "for patch, optimizer in zip(bp['boxes'], labels_box):\n",
    "    patch.set_facecolor(optimizer_colors[optimizer])\n",
    "    patch.set_alpha(0.6)\n",
    "for i, optimizer in enumerate(labels_box):\n",
    "    best_c = best_runs[optimizer]['best_C']\n",
    "    ax.scatter(i + 1, best_c, color='red', s=200, marker='*', zorder=10, edgecolors='black', linewidths=1.5)\n",
    "ax.set_ylabel('Best C Parameter', fontsize=12)\n",
    "ax.set_title('Optimal C Parameter (â˜… = Best Run)', fontsize=13, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results_img/optimizer_accuracy_separability_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved: results_img/optimizer_accuracy_separability_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ee6410",
   "metadata": {},
   "source": [
    "## 5. Loss vs Accuracy Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff47dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Loss vs Test Accuracy\n",
    "ax = axes[0, 0]\n",
    "for optimizer in optimizers:\n",
    "    final_losses = optimizer_results[optimizer]['final_losses']\n",
    "    test_accs = optimizer_results[optimizer]['test_accuracies']\n",
    "    \n",
    "    if len(final_losses) > 0:\n",
    "        ax.scatter(final_losses, test_accs, s=100, alpha=0.6,\n",
    "                   label=optimizer, color=optimizer_colors[optimizer])\n",
    "        \n",
    "        # Highlight best run\n",
    "        best_loss = best_runs[optimizer]['final_loss']\n",
    "        best_acc = best_runs[optimizer]['test_accuracy']\n",
    "        ax.scatter(best_loss, best_acc, s=300, marker='*', \n",
    "                   color=optimizer_colors[optimizer], edgecolors='black', \n",
    "                   linewidths=2, zorder=10)\n",
    "\n",
    "ax.set_xlabel('Final Loss E(Î¸)', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title('Loss vs Test Accuracy (â˜… = Best Run)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Separability Ratio vs Test Accuracy\n",
    "ax = axes[0, 1]\n",
    "for optimizer in optimizers:\n",
    "    sep_ratios = optimizer_results[optimizer]['separability_ratios']\n",
    "    test_accs = optimizer_results[optimizer]['test_accuracies']\n",
    "    \n",
    "    if len(sep_ratios) > 0:\n",
    "        ax.scatter(sep_ratios, test_accs, s=100, alpha=0.6,\n",
    "                   label=optimizer, color=optimizer_colors[optimizer])\n",
    "        \n",
    "        # Highlight best run\n",
    "        best_sep = best_runs[optimizer]['separability_ratio']\n",
    "        best_acc = best_runs[optimizer]['test_accuracy']\n",
    "        ax.scatter(best_sep, best_acc, s=300, marker='*', \n",
    "                   color=optimizer_colors[optimizer], edgecolors='black', \n",
    "                   linewidths=2, zorder=10)\n",
    "\n",
    "ax.set_xlabel('Separability Ratio', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title('Separability Ratio vs Test Accuracy (â˜… = Best Run)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Loss vs Separability Ratio\n",
    "ax = axes[1, 0]\n",
    "for optimizer in optimizers:\n",
    "    final_losses = optimizer_results[optimizer]['final_losses']\n",
    "    sep_ratios = optimizer_results[optimizer]['separability_ratios']\n",
    "    \n",
    "    if len(final_losses) > 0:\n",
    "        ax.scatter(final_losses, sep_ratios, s=100, alpha=0.6,\n",
    "                   label=optimizer, color=optimizer_colors[optimizer])\n",
    "        \n",
    "        # Highlight best run\n",
    "        best_loss = best_runs[optimizer]['final_loss']\n",
    "        best_sep = best_runs[optimizer]['separability_ratio']\n",
    "        ax.scatter(best_loss, best_sep, s=300, marker='*', \n",
    "                   color=optimizer_colors[optimizer], edgecolors='black', \n",
    "                   linewidths=2, zorder=10)\n",
    "\n",
    "ax.set_xlabel('Final Loss E(Î¸)', fontsize=12)\n",
    "ax.set_ylabel('Separability Ratio', fontsize=12)\n",
    "ax.set_title('Loss vs Separability Ratio (â˜… = Best Run)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Val Accuracy vs Test Accuracy\n",
    "ax = axes[1, 1]\n",
    "for optimizer in optimizers:\n",
    "    val_accs = optimizer_results[optimizer]['val_accuracies']\n",
    "    test_accs = optimizer_results[optimizer]['test_accuracies']\n",
    "    \n",
    "    if len(val_accs) > 0:\n",
    "        ax.scatter(val_accs, test_accs, s=100, alpha=0.6,\n",
    "                   label=optimizer, color=optimizer_colors[optimizer])\n",
    "        \n",
    "        # Highlight best run\n",
    "        best_val_acc = best_runs[optimizer]['val_accuracy']\n",
    "        best_test_acc = best_runs[optimizer]['test_accuracy']\n",
    "        ax.scatter(best_val_acc, best_test_acc, s=300, marker='*', \n",
    "                   color=optimizer_colors[optimizer], edgecolors='black', \n",
    "                   linewidths=2, zorder=10)\n",
    "\n",
    "# Add diagonal line\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Validation Accuracy', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title('Validation vs Test Accuracy (â˜… = Best Run)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0.5, 1])\n",
    "ax.set_ylim([0.5, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results_img/optimizer_correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure saved: results_img/optimizer_correlation_analysis.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
