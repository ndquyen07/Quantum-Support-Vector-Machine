{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ef00d8",
   "metadata": {},
   "source": [
    "## 1. Set Data For Training - MNIST Dataset\n",
    "\n",
    "- Dataset: MNIST (28x28 handwritten digit images)\n",
    "- Classes: 10 (digits 0-9)\n",
    "- Original features: 784 (28x28 pixels)\n",
    "- Reduced features: 4 (using PCA)\n",
    "- Random seed: 42\n",
    "- Scale: MinMaxScaler [0, 1]\n",
    "- **Total samples: 500 (subset from original 70000)**\n",
    "- **Data Split: 300 Train (60%) / 100 Validation (20%) / 100 Test (20%)**\n",
    "- **Step-by-step hyperparameter tuning using validation set**\n",
    "- SVM Pipeline with preprocessing and systematic parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80cd7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (70000, 784)\n",
      "Original number of samples: 70000\n",
      "\n",
      "Subset dataset shape: (500, 784)\n",
      "Number of samples: 500\n",
      "Number of features: 784\n",
      "Number of classes: 10\n",
      "Classes: [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "Class distribution in subset:\n",
      "Class 0: 49 samples\n",
      "Class 1: 56 samples\n",
      "Class 2: 50 samples\n",
      "Class 3: 51 samples\n",
      "Class 4: 49 samples\n",
      "Class 5: 45 samples\n",
      "Class 6: 49 samples\n",
      "Class 7: 52 samples\n",
      "Class 8: 49 samples\n",
      "Class 9: 50 samples\n",
      "\n",
      "Final split (300 train, 100 val, 100 test):\n",
      "Training samples: 300\n",
      "Validation samples: 100\n",
      "Test samples: 100\n",
      "\n",
      "Applying PCA for dimensionality reduction...\n",
      "0.4507376443161647\n",
      "Explained variance ratio: [0.09909844 0.07003463 0.06263133 0.05757995 0.05025231 0.04532871\n",
      " 0.03584532 0.02996696]\n",
      "Total explained variance: 0.4507\n",
      "\n",
      "Data splits:\n",
      "Training set: (300, 8) (300 samples, 60%)\n",
      "Validation set: (100, 8) (100 samples, 20%)\n",
      "Test set: (100, 8) (100 samples, 20%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# Load MNIST dataset (28x28 handwritten digit images)\n",
    "mnist = datasets.fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X = mnist.data.to_numpy() if hasattr(mnist.data, 'to_numpy') else mnist.data\n",
    "y = mnist.target.astype(int).to_numpy() if hasattr(mnist.target, 'to_numpy') else mnist.target.astype(int)\n",
    "print(f\"Original dataset shape: {X.shape}\")\n",
    "print(f\"Original number of samples: {X.shape[0]}\")\n",
    "\n",
    "# Limit to 500 samples for training/validation/test\n",
    "X_subset, _, y_subset, _ = train_test_split(\n",
    "    X, y, train_size=500, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nSubset dataset shape: {X_subset.shape}\")\n",
    "print(f\"Number of samples: {X_subset.shape[0]}\")\n",
    "print(f\"Number of features: {X_subset.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_subset))}\")\n",
    "print(f\"Classes: {np.unique(y_subset)}\")\n",
    "\n",
    "# Show sample distribution\n",
    "print(\"\\nClass distribution in subset:\")\n",
    "unique, counts = np.unique(y_subset, return_counts=True)\n",
    "for i, count in zip(unique, counts):\n",
    "    print(f\"Class {i}: {count} samples\")\n",
    "\n",
    "# Split data into train/validation/test sets\n",
    "# First split: 300 train (60%), 200 temp (40%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_subset, y_subset, train_size=300, random_state=42, stratify=y_subset\n",
    ")\n",
    "\n",
    "# Second split: 100 validation, 100 test (from the 200 temp)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal split (300 train, 100 val, 100 test):\")\n",
    "print(f\"Training samples: {len(y_train)}\")\n",
    "print(f\"Validation samples: {len(y_val)}\")\n",
    "print(f\"Test samples: {len(y_test)}\")\n",
    "\n",
    "\n",
    "# Normalize features to [0,1]\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# PCA for dimensionality reduction (from 784 to 4 dimensions)\n",
    "print(\"\\nApplying PCA for dimensionality reduction...\")\n",
    "pca = PCA(n_components=8)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"Training set: {X_train.shape} (300 samples, 60%)\")\n",
    "print(f\"Validation set: {X_val.shape} (100 samples, 20%)\")\n",
    "print(f\"Test set: {X_test.shape} (100 samples, 20%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62fc8592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ../data/mnist_8features_data.npz\n"
     ]
    }
   ],
   "source": [
    "# Save processed data\n",
    "np.savez_compressed(\n",
    "\t\"../data/mnist_8features_data.npz\",\n",
    "\tX_train=X_train,\n",
    "\tX_val=X_val,\n",
    "\tX_test=X_test,\n",
    "\ty_train=y_train,\n",
    "\ty_val=y_val,\n",
    "\ty_test=y_test\n",
    ")\n",
    "print(\"Data saved to ../data/mnist_8features_data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e47e159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shapes:\n",
      "X_train: (300, 8), y_train: (300,)\n",
      "X_val: (100, 8), y_val: (100,)\n",
      "X_test: (100, 8), y_test: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Load processed MNIST data\n",
    "data = np.load(\"../data/mnist_8features_data.npz\")\n",
    "\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "print(f\"Loaded data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "315adda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "339cee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc | Test acc\n",
      "0.79 | 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from src.utils import calculate_accuracy\n",
    "\n",
    "rbf_K_train = rbf_kernel(X_train)\n",
    "rbf_K_val = rbf_kernel(X_val, X_train)\n",
    "rbf_K_test = rbf_kernel(X_test, X_train)\n",
    "\n",
    "classical_val_acc, classical_test_acc, _ = calculate_accuracy(\n",
    "    rbf_K_train, rbf_K_val, rbf_K_test,\n",
    "    y_train, y_val, y_test\n",
    ")\n",
    "print(\"Val acc | Test acc\")\n",
    "print(f\"{classical_val_acc} | {classical_test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svqsvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
